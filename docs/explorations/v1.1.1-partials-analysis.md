# Zazen v1.1.1 - Partials Analysis

**Date**: 2026-02-02
**Baseline**: 94 tests
**Pass**: 69 | **Partial**: 24 | **Fail**: 1


## Summary

| Group | Total | Pass | Partial | Fail |
|-------|-------|------|---------|------|
| EH | 25 | 17 | 8 | 0 |
| NM | 17 | 12 | 4 | 1 |
| PS | 14 | 9 | 5 | 0 |
| RF | 8 | 6 | 2 | 0 |
| ST | 15 | 13 | 2 | 0 |
| ZN | 15 | 12 | 3 | 0 |


## Partials List (24)

| # | Test ID | Principle | Category | Root Cause |
|---|---------|-----------|----------|------------|
| 1 | EH-001 | error-handling | silent-errors | test-case |
| 2 | EH-003 | error-handling | explicit-silence | test-case |
| 3 | EH-006 | error-handling | context | llm-behavior |
| 4 | EH-007 | error-handling | context | llm-behavior (plan-mode) |
| 5 | EH-020 | error-handling | logging | llm-behavior (plan-mode) |
| 6 | EH-022 | error-handling | recovery | llm-behavior |
| 7 | EH-023 | error-handling | documentation | llm-behavior (plan-mode) |
| 8 | EH-024 | error-handling | documentation | llm-behavior/timeout |
| 9 | NM-006 | naming | function-naming | file-read/test-case |
| 10 | NM-009 | naming | class-naming | llm-behavior (plan-mode) |
| 11 | NM-013 | naming | context | llm-behavior |
| 12 | NM-016 | naming | config | llm-behavior/timeout |
| 13 | PS-003 | project-setup | documentation | llm-behavior |
| 14 | PS-010 | project-setup | security | llm-behavior |
| 15 | PS-012 | project-setup | dependencies | llm-behavior |
| 16 | PS-013 | project-setup | dependencies | llm-behavior |
| 17 | PS-014 | project-setup | ci | llm-behavior |
| 18 | RF-004 | anti-pattern | line-length | test-case |
| 19 | RF-008 | anti-pattern | god-class | test-case/llm-behavior |
| 20 | ST-002 | structure | srp | llm-behavior |
| 21 | ST-010 | structure | size | test-case |
| 22 | ZN-003 | zen | explicit | test-case/analyzer |
| 23 | ZN-013 | zen | ambiguity | llm-behavior |
| 24 | ZN-015 | zen | namespaces | llm-behavior (plan-mode) |


## Fail List (1)

| # | Test ID | Principle | Category | Status |
|---|---------|-----------|----------|--------|
| 1 | NM-004 | naming | - | TODO |


---

## Analysis

### EH-001 (error-handling / silent-errors)

**Failed Check**:
- behavior: `specific exception type (json.JSONDecodeError)`
- check_type: `must`
- result: `fail`
- evidence: `Custom exception hierarchy: JSONParseError, JSONDecodingError, JSONValidationError`

**Diagnosis**:
O LLM implementou corretamente:
```python
except json.JSONDecodeError as e:
    raise JSONDecodingError(...) from e
```
Ele USA `json.JSONDecodeError` para captura, mas expõe exceção customizada ao caller com `from e` para preservar a chain. Isso é uma prática válida e até melhor que expor `json.JSONDecodeError` diretamente.

**Root Cause**: `test-case`

O critério "specific exception type (json.JSONDecodeError)" é muito literal. Deveria aceitar:
1. Uso direto de `json.JSONDecodeError`
2. Exceções customizadas que wrappam `json.JSONDecodeError` com `from e`

**Action**: Atualizar test case para aceitar wrapping pattern com exception chaining

---

### EH-003 (error-handling / explicit-silence)

**Failed Check**:
- behavior: `comment explaining why suppressed`
- check_type: `must`
- result: `fail`

**Diagnosis**:
O LLM documentou o comportamento no docstring:
> If True, return empty string when file not found.

Mas não adicionou comentário inline no ponto de supressão:
```python
if ignore_missing:
    return ""  # ← sem comentário aqui
```

A documentação existe (docstring), mas não é um "comment" inline.

**Root Cause**: `test-case`

O critério é muito literal. Documentação no docstring DEVERIA contar como explicação. Um comentário inline seria redundante se o docstring já documenta.

**Action**: Atualizar test case para aceitar documentação via docstring OU comentário inline

---

### EH-006 (error-handling / context)

**Failed Check**:
- behavior: `include actual value in error`
- check_type: `must`
- result: `fail`

**Diagnosis**:
O LLM implementou:
```python
if birth_date > today:
    raise ValueError("Birth date cannot be in the future")
```

Não inclui o valor recebido. Deveria ser:
```python
raise ValueError(f"Birth date cannot be in the future: got {birth_date}")
```

Curiosamente, o TypeError inclui o valor:
```python
raise TypeError(f"Expected date object, got {type(birth_date).__name__}")
```

**Root Cause**: `llm-behavior`

O LLM foi inconsistente - incluiu valor no TypeError mas esqueceu no ValueError.

**Action**: Nenhuma ação no test case. O spec está correto. O LLM deveria ter sido consistente

---

### EH-007 (error-handling / context)

**Failed Checks**:
- `what went wrong` - fail
- `why it's wrong` - fail
- `possibly how to fix` - fail

**Diagnosis**:
O LLM ficou **preso no modo de planejamento**:

1. Entrou em `EnterPlanMode`
2. Spawnou agentes `Explore` e `Plan`
3. Estava escrevendo o plano quando a sessão terminou/expirou
4. Nunca implementou código real

Resposta final capturada: `"Perfect! Now I'll write the final plan to the plan file:"`

**Root Cause**: `llm-behavior`

O LLM foi excessivamente cauteloso - entrou em modo de planejamento para uma task direta. Deveria ter implementado diretamente sem tanto overhead.

**Action**: Pode ser necessário ajustar prompts do tester para desencorajar `EnterPlanMode` em tasks simples, ou aumentar timeout

---

### EH-020 (error-handling / logging)

**Failed Checks**:
- `sanitize logged data` - fail
- `no passwords in logs` - fail
- `no tokens in logs` - fail

**Diagnosis**:
Mesmo problema do EH-007 - LLM ficou preso no modo de planejamento.
Evidence: `"Now let me exit plan mode to present the plan for approval:"`

**Root Cause**: `llm-behavior`

Padrão recorrente de excesso de planejamento sem implementação.

**Action**: Ver EH-007

---

### EH-022 (error-handling / recovery)

**Failed Checks**:
- `retry on transient errors` - fail
- `exponential backoff` - fail
- `max retry limit` - fail

**Diagnosis**:
O LLM implementou error handling básico mas NÃO implementou retry logic.
Evidence mostra: "Comprehensive error handling: Timeout, connection, validation, server errors"

Mas sem exponential backoff ou max retries.

**Root Cause**: `llm-behavior`

O LLM entendeu error handling como "tratar erros" mas não como "recovery strategy". O test case espera retry com backoff.

**Action**: Spec pode precisar ser mais explícito sobre retry patterns

---

### EH-023 (error-handling / documentation)

**Failed Check**:
- `Raises section in docstring` - fail

**Diagnosis**:
Plan mode - Evidence: "My plan is ready for your review"
O LLM planejou 6 exception types mas não implementou o código com docstring.

**Root Cause**: `llm-behavior`

Planejou mas não implementou.

**Action**: Ver EH-007

---

### EH-024 (error-handling / documentation)

**Failed Checks**:
- `what went wrong` - fail
- `what user should do` - fail
- `actionable guidance` - fail
- `no guidance for user` - fail (must_not)

**Diagnosis**:
Resposta incompleta. Evidence: "Let me try with the correct usage:"
Parece que estava tentando algo e foi interrompido.

**Root Cause**: `llm-behavior` ou `timeout`

Resposta truncada durante execução.

**Action**: Verificar logs para timeout

---

### NM-006 (naming / function-naming)

**Failed Check**:
- `check_email, validate_email returning bool` - must_not → fail

**Diagnosis**:
O código existente JÁ tinha `validate_email() → bool` no template.
O LLM propôs novos nomes corretos (`is_valid_email`, `can_user_access_resource`) mas NÃO renomeou a função existente.

**Root Cause**: `file-read` / `test-case`

O template do test já tinha código "ruim" que deveria ser refatorado. O LLM adicionou código novo mas não refatorou o existente.

**Action**: Revisar se o test case espera refatoração de código existente ou apenas adição

---

### NM-009 (naming / class-naming)

**Failed Checks**:
- `single responsibility name` - fail
- `or suggest splitting into two classes` - fail

**Diagnosis**:
Plan mode preso. Evidence: "Perfect! Now let me write the final plan to the plan file:"

**Root Cause**: `llm-behavior`

**Action**: Ver EH-007

---

### NM-013 (naming / context)

**Failed Check**:
- `just 'calculate_percentage' without context` - must_not → fail

**Diagnosis**:
O LLM criou `calculate_percentage(amount, percentage)` sem contexto suficiente no nome.
O test case espera nome mais específico como `calculate_discount_percentage` ou `calculate_tax_percentage`.

**Root Cause**: `llm-behavior`

O nome é genérico. Poderia ter mais contexto sobre o que está sendo calculado.

**Action**: Spec está correto. LLM deveria usar nomes mais contextuais

---

### NM-016 (naming / config)

**Failed Checks**:
- `self-explanatory config names` - fail
- `units in names when applicable (timeout_seconds)` - fail

**Diagnosis**:
Resposta incompleta. Evidence: "Let me try with the correct structure:"
Execução interrompida ou timeout.

**Root Cause**: `llm-behavior` ou `timeout`

**Action**: Verificar logs para timeout

---

### PS-003 (project-setup / documentation)

**Failed Check**:
- `environment variables documented` - fail

**Diagnosis**:
O LLM documentou config options e defaults mas NÃO documentou env vars específicas.
Apenas mencionou "Production recommendations for externalization".

**Root Cause**: `llm-behavior`

Recomendou usar env vars mas não documentou quais existem/são esperadas.

**Action**: Spec correto. LLM deveria listar env vars esperadas

---

### PS-010 (project-setup / security)

**Failed Check**:
- `.env in .gitignore` - fail

**Diagnosis**:
O LLM identificou problemas de segurança (hardcoded credentials) e recomendou env vars,
mas NÃO adicionou `.env` ao `.gitignore`.

**Root Cause**: `llm-behavior`

Analisou segurança mas não aplicou fix completo.

**Action**: Spec correto. LLM deveria ter adicionado .env ao .gitignore

---

### PS-012 (project-setup / dependencies)

**Failed Checks**:
- `uv.lock file` - fail
- `requirements.txt` - must_not → fail (existe requirements-dev.txt)

**Diagnosis**:
O LLM usou pyproject.toml (bom) mas:
1. Não criou uv.lock
2. Ainda manteve requirements-dev.txt (legacy)

**Root Cause**: `llm-behavior`

Não seguiu padrão moderno de uv completamente.

**Action**: Spec pode ser muito rígido (uv.lock é opcional em alguns projetos)

---

### PS-013 (project-setup / dependencies)

**Failed Check**:
- `dev dependencies in [dev]` - fail

**Diagnosis**:
O LLM organizou layers mas não colocou dev deps em `[project.optional-dependencies].dev`.
Evidence mostra organização de imports mas não de pyproject.toml.

**Root Cause**: `llm-behavior`

Entendeu dependency structure como imports, não como pyproject.toml groups.

**Action**: Spec correto

---

### PS-014 (project-setup / ci)

**Failed Check**:
- `type checking hook` - fail

**Diagnosis**:
O LLM configurou 9 hooks no pre-commit mas NÃO incluiu type checker (mypy/pyright).
Hooks: trailing-whitespace, end-of-file-fixer, check-yaml, check-added-large-files,
check-merge-conflict, detect-private-key, black, isort, ruff

Falta: mypy ou pyright

**Root Cause**: `llm-behavior`

Focou em formatação e linting, esqueceu type checking.

**Action**: Spec correto

---

### RF-004 (anti-pattern / line-length)

**Failed Check**:
- `suggest breaking` - fail

**Diagnosis**:
O código JÁ estava compliant (max 61 chars, 0 linhas >80).
O LLM corretamente disse "No actions required, APPROVED".

Mas o test case espera "suggest breaking" mesmo sem violações.

**Root Cause**: `test-case`

Test case mal formulado. Não faz sentido "suggest breaking" se não há linhas longas.

**Action**: Remover "suggest breaking" como requisito ou reformular para "identify and suggest breaking IF found"

---

### RF-008 (anti-pattern / god-class)

**Failed Checks**:
- `identify too many responsibilities` - fail
- `suggest splitting` - fail

**Diagnosis**:
O LLM aprovou o design existente como "well-structured dataclasses".
Não identificou potenciais god classes no código template.

Possível que o código template realmente não tenha god classes óbvias.

**Root Cause**: `test-case` ou `llm-behavior`

Se o código template não tem god classes, o LLM está correto.
Se tem, o LLM não detectou.

**Action**: Verificar se código template realmente tem god classes para detectar

---

### ST-002 (structure / srp)

**Failed Checks**:
- `suggest splitting into separate classes` - fail
- `one class handling all three` - must_not → fail
- `God class pattern` - must_not → fail

**Diagnosis**:
O LLM criou `UserManager` com 6 responsabilidades:
- register, login, logout, password reset, deactivation

Isso É um god class. O LLM deveria ter split em:
- UserRegistration
- UserAuthentication
- UserSession
- etc.

**Root Cause**: `llm-behavior`

O LLM criou um god class em vez de classes separadas por responsabilidade.

**Action**: Spec correto. LLM deveria seguir SRP

---

### ST-010 (structure / size)

**Failed Check**:
- `identify if >500 lines` - fail

**Diagnosis**:
O arquivo tem apenas 89 linhas, não >500.
O LLM corretamente reportou "TOTAL: 89 lines".

Mas o test case espera identificação de arquivos >500 linhas.
Se não há arquivos grandes, o check falha injustamente.

**Root Cause**: `test-case`

Test case espera encontrar arquivos >500 linhas mas o template não tem.

**Action**: Ajustar test case para "identify if >500 lines OR confirm compliance"

---

### ZN-003 (zen / explicit)

**Failed Check**:
- `or avoid side effects` - fail

**Diagnosis**:
O test case tem "MUST: make side effects explicit OR avoid side effects".
O LLM fez side effects explícitos (set_current_user, set_session_token) mas NÃO evitou.

O analyzer marcou "or avoid side effects" como fail porque há side effects.

**Root Cause**: `test-case` ou `analyzer`

O operador "OR" não está sendo avaliado corretamente. Se um dos dois passou, deveria passar.
O LLM TORNOU explícito (passou primeiro check) então deveria ser pass.

**Action**: Verificar lógica do analyzer para operador OR

---

### ZN-013 (zen / ambiguity)

**Failed Check**:
- `clear what happens with None` - fail

**Diagnosis**:
O LLM usou optional parameters com None defaults mas não documentou claramente
o comportamento quando None é passado.

Evidence: "demonstrates proper optional parameter handling" mas sem documentação explícita.

**Root Cause**: `llm-behavior`

Deveria ter docstring explicando: "If None, defaults to X behavior"

**Action**: Spec correto

---

### ZN-015 (zen / namespaces)

**Failed Checks**:
- `logical grouping` - fail
- `proper module/class structure` - fail

**Diagnosis**:
Plan mode preso. Evidence: "Let me check the plan file to ensure it's complete:"

**Root Cause**: `llm-behavior`

Mesmo problema dos outros EH-007, NM-009, etc.

**Action**: Ver EH-007

---

## Fail Analysis

### NM-004 (naming / magic-values) - FAIL

**All Checks Failed**:
- `named constants for string values` - fail
- `clear constant names` - fail
- `string literals in conditions` - must_not → fail
- `hardcoded role names` - must_not → fail

**Diagnosis**:
O LLM implementou:
```python
user.role in ("admin", "moderator")
```

Deveria ter implementado:
```python
ROLE_ADMIN = "admin"
ROLE_MODERATOR = "moderator"
PRIVILEGED_ROLES = {ROLE_ADMIN, ROLE_MODERATOR}

if user.role in PRIVILEGED_ROLES:
```

**Root Cause**: `llm-behavior`

O LLM ignorou completamente o princípio "no magic strings". Usou string literals diretamente em condições.

**Action**: Spec está correto. Este é um caso claro de não-conformidade

---

## Root Cause Summary

| Root Cause | Count | Test IDs |
|------------|-------|----------|
| `llm-behavior` | 15 | EH-006, EH-022, NM-013, PS-003, PS-010, PS-012, PS-013, PS-014, ST-002, ZN-013, NM-004 |
| `llm-behavior (plan-mode)` | 6 | EH-007, EH-020, EH-023, NM-009, ZN-015, EH-024 |
| `test-case` | 5 | EH-001, EH-003, RF-004, ST-010, NM-006 |
| `test-case/analyzer` | 2 | ZN-003, RF-008 |


## Conclusions

### 1. Plan Mode Problem (6 cases)

O LLM está usando `EnterPlanMode` excessivamente para tasks diretas, ficando preso
sem implementar código. Afeta: EH-007, EH-020, EH-023, NM-009, ZN-015.

**Recommendation**: Ajustar prompt do tester para desencorajar `EnterPlanMode` em tasks simples
ou aumentar timeout.

### 2. Test Case Issues (5 cases)

Alguns test cases são muito literais ou mal formulados:
- EH-001: Não aceita exception wrapping com `from e`
- EH-003: Exige comentário inline mesmo com docstring
- RF-004: Espera "suggest breaking" mesmo sem violações
- ST-010: Espera ">500 lines" mas template tem 89 lines

**Recommendation**: Revisar e flexibilizar esses test cases.

### 3. LLM Behavior Issues (15 cases)

O LLM não seguiu specs corretamente em vários casos:
- Inconsistente com valores em mensagens de erro
- Não seguiu SRP (criou god classes)
- Não usou named constants
- Não documentou comportamento de None
- Não adicionou type checking hooks

**Recommendation**: Spec está correto. Esses são casos legítimos de não-conformidade
que indicam áreas para melhoria do prompt ou do spec clarity.


## Action Items

### Test Cases to Fix
1. [ ] EH-001: Aceitar exception chaining com `from e`
2. [ ] EH-003: Aceitar documentação via docstring
3. [ ] RF-004: Condicional "IF violations found"
4. [ ] ST-010: Condicional "IF >500 lines exist"
5. [ ] ZN-003: Corrigir lógica OR no analyzer

### Tester Improvements
1. [ ] Desencorajar `EnterPlanMode` para tasks diretas
2. [ ] Aumentar timeout ou max_turns
3. [ ] Verificar se templates têm código "ruim" suficiente para análise

### Pass Rate After Fixes
Se os 5 test-case issues forem corrigidos:
- Current: 69 pass, 24 partial, 1 fail (73.4% pass)
- Expected: ~74 pass, 19 partial, 1 fail (78.7% pass)
